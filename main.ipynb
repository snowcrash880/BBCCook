{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, time, datetime\n",
    "import os, re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import sqrt\n",
    "from math import log\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DATA GATHERING FUNCTIONS\n",
    "\n",
    "\n",
    "# Extract informations from recipe url\n",
    "\n",
    "# Linguistic preprocessing of recipe, see NLTK\n",
    "def processRecipe(path):\n",
    "    \"\"\"\n",
    "    Stopword removal, normalization, stemmings.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    stop = stopwords.words('english')\n",
    "    with open(path, encoding = \"utf-8\") as f:\n",
    "        s = f.read()\n",
    "\n",
    "    l = word_tokenize(s)\n",
    "    l = [i for word in l for i in word.split(\".\") if i]\n",
    "    l = [ps.stem(re.sub(r'[^a-zA-Z]', '', i.lower())) for i in word_tokenize(s) if i not in stop ]\n",
    "    l = list(filter(None, l))\n",
    "    l = [i for i in l if len(i)>2]\n",
    "    \n",
    "    return l\n",
    "\n",
    "# Term frequency in document\n",
    "def tf(path):\n",
    "    '''Compute the term frequency of a word in a document as\n",
    "    the number of the term appear in the document divided the total \n",
    "    count of the word in the document.'''\n",
    "    freq = {}\n",
    "    l = processRecipe(path)    # the document is preprocessed \n",
    "    tot_count = len(l)    # so we don't keep in consideration stopword etc.\n",
    "    for word in l:\n",
    "        if word in freq:\n",
    "            freq[word] += 1\n",
    "        if word not in freq:\n",
    "            freq[word] = 1\n",
    "\n",
    "    for key in freq.keys():\n",
    "        freq[key] = round(freq[key]/tot_count,4)\n",
    "    \n",
    "    return freq\n",
    "\n",
    "\n",
    "# Compile dictionary used to write vocabulary and index\n",
    "def recipeDict():\n",
    "    '''Create a dictionary who has parsed recipes keywords has keys,\n",
    "    a list of lists containing [path_to_file in which keyword appear, \n",
    "    frequency of the word in the document, position]. The frequency is\n",
    "    the tf (term frequency)'''\n",
    "    my_dict = {}\n",
    "    for file_path in os.listdir(\"recipes/\"):\n",
    "        freq = tf(\"recipes/\"+file_path)\n",
    "        word_list = processRecipe(\"recipes/\"+file_path)\n",
    "        \n",
    "        for word in set(word_list):\n",
    "            pos = [n for n, i in enumerate(word_list) if i == word]\n",
    "            if word in my_dict.keys():\n",
    "                my_dict[word].append([file_path.strip(\".txt\"), freq[word], pos]) \n",
    "                                      \n",
    "            if word not in my_dict.keys():\n",
    "                my_dict[word] = [[file_path.strip(\".txt\"),freq[word], pos]]\n",
    "            \n",
    "    return my_dict\n",
    "\n",
    "# Add skip pointers to index\n",
    "def add_skip(index):\n",
    "    '''Add skip to posting lists. Skip has step equal to square root\n",
    "    of posting list length.'''\n",
    "    for term in index.keys():\n",
    "        pos_length = len(index[term])\n",
    "        step = int(sqrt(pos_length))\n",
    "        for n in range(pos_length):\n",
    "            if n in range(0, pos_length-step, step): index[term][n].insert(2, n+step)\n",
    "            else : index[term][n].insert(2, 0)\n",
    "\n",
    "\n",
    "def vocabulary(my_dict):\n",
    "    '''Write the vocabulary on disk, each term has associated termID\n",
    "    and overall frequency (# documents occur / tot doc)'''\n",
    "    word_list = list(my_dict.keys())\n",
    "    word_list.sort()\n",
    "    word_num = len(os.listdir(\"./recipes/\"))\n",
    "    with open(\"vocabulary.txt\",\"w\") as f:\n",
    "        i = 0\n",
    "        for word in word_list:\n",
    "            idf = log(word_num/len(my_dict[word]))\n",
    "            f.write(word+\"\\t\" +str(i).zfill(len(str(word_num)))+ \"\\t\" +str(round(idf,3))+\"\\n\")\n",
    "            i+=1\n",
    "            \n",
    "# Write the index on disk\n",
    "def index(my_dict):\n",
    "    '''Write the index on disk.'''\n",
    "    vocabulary = {}\n",
    "    with open(\"vocabulary.txt\") as f:\n",
    "        for line in f:\n",
    "            l = line.split()\n",
    "            vocabulary[l[0]] = (l[1],l[2])\n",
    "    g = open(\"index.txt\", \"w\")\n",
    "    for word in vocabulary.keys():\n",
    "        g.write(vocabulary[word][0]+\"\\t\")\n",
    "        for file_ref in my_dict[word]:\n",
    "            pos = \"-\".join([str(i) for i in file_ref[3]])\n",
    "            tfidf = str( round( file_ref[1]*float(vocabulary[word][1]),3 ) )\n",
    "            g.write(file_ref[0]+ \" \" + tfidf + \" \" + str(file_ref[2]) + \" \" + pos +\"\\t\")\n",
    "        g.write(\"\\n\")\n",
    "    g.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### FUNCTIONS TO RECOVER DATA FROM THE HARD-DISK\n",
    "\n",
    "def retrieveRecipe(rec_num):\n",
    "    '''Retrieve a recipe from the disk give the recipe number.'''\n",
    "    headers = [\"Title\", \"Author\", \"CookTime\", \"Prep_Time\", \n",
    "                 \"Serves\", \"Description\", \"Dietary\", \"Instructions\", \n",
    "                 \"Ingredient_list\"]\n",
    "    path = \"recipes/\"+str(rec_num).zfill(5)+ \".txt\"\n",
    "    rec = pd.read_csv(path, sep = \"\\t\", header = None)\n",
    "\n",
    "    rec = rec.rename(columns = {i:headers[i] for i in range(0,8)})\n",
    "    rec = rec.rename(columns = {i:\"Ingredients\" \n",
    "                      for i in range(8,rec.shape[1])})\n",
    "    return rec\n",
    "\n",
    "def loadVocabulary():\n",
    "    '''Load Vocabulary File'''\n",
    "    vocabulary = {}\n",
    "    with open(\"vocabulary.txt\") as f:\n",
    "        for line in f:\n",
    "            l = line.split()\n",
    "            vocabulary[l[0]] = (l[1], l[2])\n",
    "    return vocabulary\n",
    "\n",
    "def loadIndex():\n",
    "    '''Load Index file.'''\n",
    "    index = {}\n",
    "    with open(\"index.txt\") as f:\n",
    "        for line in f:\n",
    "            l = line.split(\"\\t\")\n",
    "            l = [i.strip() for i in l if i.strip()]\n",
    "            l1 = [i.split() for i in l[1:]]\n",
    "            for i in range(len(l1)):    \n",
    "                pos = [int(i) for i in l1[i][3].split(\"-\")]\n",
    "                l1[i].pop(3)\n",
    "                l1[i].append(pos)\n",
    "            index[l[0]] = l1 \n",
    "    return index\n",
    "\n",
    "def posting(term, index, vocabulary):\n",
    "    '''Take a term and return its posting list.'''\n",
    "    term_num = vocabulary[term][0]\n",
    "    return index[term_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### FUNCTIONS TO PERFORM SEARCH AND OUTPUT RESULTS\n",
    "\n",
    "# Intersection of two posting lists\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## BUILDING COLLECTION\n",
    "\n",
    "getIngredients(write = True)\n",
    "getRecipeLinks(ingredients[304:], delay = 0, verbose = True)\n",
    "\n",
    "## Load Ingredient List\n",
    "\n",
    "with open(\"./ingredient_list.txt\") as f:\n",
    "    ingredients = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "## Load recipesAddresses\n",
    "\n",
    "with open(\"./recipe_links.txt\") as f:\n",
    "    recipes_add = list(set([line.strip() for line in f if line.strip()]))\n",
    "\n",
    "buildCollection(recipes_add, delay = 1)\n",
    "\n",
    "# Make dictionary and add skipping list. Then create vocabulary and index.\n",
    "rec_dic = recipeDict()  # 3.5 minuti\n",
    "add_skip(rec_dic)\n",
    "vocabulary(rec_dic)\n",
    "index(rec_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Index...\n",
      "\n",
      "Index loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Load Index and vocabulary\n",
    "\n",
    "print(\"Loading Index...\\n\")\n",
    "ind = loadIndex()    # load index in memory\n",
    "voc = loadVocabulary()    # load vocabulary in memory\n",
    "X = (ind, voc)    # create ind,voc tuple\n",
    "print(\"Index loaded.\\n\")\n",
    "\n",
    "## SEARCH INTERFACE \n",
    "\n",
    "print(\"Search : \",end = '')\n",
    "query = str(input( ))    # input query\n",
    "\n",
    "searchResults = search(query, *X)\n",
    "docIDResults  = [d[0] for d in searchResults]\n",
    "\n",
    "print(\"Found \"+str(len(docIDResults))+ \" results.\\n\")\n",
    "\n",
    "max_results = 10    # set max number of result to output\n",
    "\n",
    "results = cosSim(query, docIDResults, *X)[:max_results]\n",
    "printResults(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
