{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, time, datetime\n",
    "import os, re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import sqrt\n",
    "from math import log\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DATA GATHERING FUNCTIONS\n",
    "\n",
    "def requestURL(url):\n",
    "    '''Request routine to url. Catch error and keep the connection active'''\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(url, timeout = 5)    \n",
    "            r.raise_for_status()\n",
    "        #except requests.RequestException:\n",
    "        #    time.sleep(5)\n",
    "        #    continue\n",
    "        except requests.Timeout:\n",
    "            print(\"\\nTimeout!\\n\")\n",
    "            continue\n",
    "        \n",
    "        except requests.ConnectionError:\n",
    "            print(\"\\nConnectionError!\\n\")\n",
    "            continue\n",
    "    \n",
    "        except requests.HTTPError:\n",
    "            print(\"\\nHTTPError!\\n\")\n",
    "            continue\n",
    "        break\n",
    "    return r\n",
    "\n",
    "\n",
    "def getIngredients(delay = 1, write = False):\n",
    "    '''Explore the bbc site to capture all ingredients.\n",
    "    If write is True, the ingredient list is written in ingredient_list.txt.\n",
    "    '''\n",
    "    \n",
    "    BASE_URL = \"http://www.bbc.co.uk\"\n",
    "    r = requestURL(BASE_URL+\"/food/ingredients\")\n",
    "    \n",
    "    Soup = BeautifulSoup(r.text,\"lxml\")\n",
    "    \n",
    "    p = re.compile(\"/food/ingredients/by/letter/*\") # create regular expression\n",
    "    ing_pag = [BASE_URL+i.get(\"href\") for i in Soup.find_all(\"a\", {\"href\" :p})]\n",
    "\n",
    "    ingredients = []\n",
    "    for link in ing_pag:\n",
    "        r = requestURL(link)\n",
    "        Soup = BeautifulSoup(r.text,\"lxml\")\n",
    "        l = [\n",
    "            i.get(\"id\") for i \n",
    "            in Soup.find_all(\"li\", {\"class\":\"resource food\"})]\n",
    "        ingredients.extend(l)\n",
    "        time.sleep(delay)\n",
    "    if write:    # optionally write the ingredient-list on file\n",
    "        with open(\"ingredient_list.txt\", \"w\") as f:\n",
    "            for ing in ingredients:\n",
    "                f.write(ing+\"\\n\")\n",
    "    else:    \n",
    "        return ingredients\n",
    "\n",
    "\n",
    "def extractRecipe(ingredient, l, delay = 0, verbose = False):\n",
    "    '''Extract all recipes addresses from one ingredient. The\n",
    "    url are saved in l list.'''\n",
    "    \n",
    "    BASE_URL = \"http://www.bbc.co.uk\"\n",
    "    ingredient = \"+\".join(re.split(\"_\", ingredient))\n",
    "    url = BASE_URL+\"/food/recipes/search?&keywords=\"+ingredient\n",
    "    \n",
    "    r = requestURL(url)\n",
    "    \n",
    "    \n",
    "    Soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    p = re.compile(\"/food/recipes/search*\")\n",
    "    Next_page = Soup.find_all(\"a\", {\"href\":p})\n",
    "    if Next_page == [] : pages_number = 1\n",
    "    else: pages_number = max([int(i.string) for i in Next_page if i.string != \"Next\"])\n",
    "    \n",
    "    p = re.compile(\"/food/recipes/*\") # reg express\n",
    "    \n",
    "    if verbose: print(\"< \"+ingredient+\" > Start fetching. Page: \") # verbose\n",
    "    \n",
    "    for page in range(1,pages_number+1):\n",
    "        r = requestURL(BASE_URL+\"/food/recipes/\"+\"search?page=\"+str(page)+\"&keywords=\"+ingredient)\n",
    "        Soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        l_temp = [BASE_URL+i.get(\"href\") for i in Soup.find_all(\"a\", {\"href\":p})if \"search\" not in i.get(\"href\") \n",
    "                 if len(BASE_URL)+1 < len(i.get(\"href\"))\n",
    "                ]\n",
    "        l.extend(l_temp)\n",
    "        if verbose: print(page, end = \" \")\n",
    "        time.sleep(delay)\n",
    "\n",
    "\n",
    "def getRecipeLinks(ingredients, delay = 1, verbose = False):\n",
    "    \"\"\"\n",
    "    Starting from a list of ingredients crawls http://www.bbc.co.uk/food\n",
    "    to collect all linked recipes links.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ingredients : list of str\n",
    "    delay : int\n",
    "        seconds of delay between requests to site\n",
    "    \"\"\"\n",
    "    for ingredient_name in ingredients:\n",
    "        recipes_address = []\n",
    "        extractRecipe(ingredient_name, recipes_address, delay, verbose)\n",
    "        f = open(\"recipe_links.txt\", \"a\")\n",
    "        for add in recipes_address:\n",
    "            f.write(add+\"\\n\")\n",
    "        if verbose: print(\"< \"+ingredient_name+\" > complete!\")\n",
    "        f.close()\n",
    "\n",
    "\n",
    "# Extract informations from recipe url\n",
    "def parseRecipe(url):\n",
    "    \"\"\"\n",
    "    Parse a recipe from http://www.bbc.co.uk/food.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : \n",
    "    Returns\n",
    "    -------\n",
    "    recipe : dict\n",
    "        dictionary containing Title, Author, CookTime, Prep_Time, Serves,\n",
    "        Description, Dietary, Ingredient_list, Instructions\n",
    "    \"\"\"\n",
    "    \n",
    "    r = requestURL(url)\n",
    "    Soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    recipe = {}\n",
    "    \n",
    "    Title = Soup.find(\"meta\", {\"property\":\"og:title\"})\n",
    "    recipe[\"Title\"] = Title.get(\"content\") if Title else \"Empty\" \n",
    "\n",
    "    Author = Soup.find(\"a\", {\"class\":\"chef__link\"})\n",
    "    recipe[\"Author\"] = Author.text if Author else \"Empty\"\n",
    "\n",
    "    CookTime = Soup.find(\"p\", {\"class\":\"recipe-metadata__cook-time\"})\n",
    "    recipe[\"CookTime\"] = CookTime.text if CookTime else \"Empty\"\n",
    "    \n",
    "    Prep_Time = Soup.find(\"p\", {\"class\":\"recipe-metadata__prep-time\"})\n",
    "    recipe[\"Prep_Time\"] = Prep_Time.text if Prep_Time else \"Empty\"\n",
    "        \n",
    "    Serves = Soup.find(\"p\", {\"class\":\"recipe-metadata__serving\"})\n",
    "    recipe[\"Serves\"] = Serves.text if Serves else \"Empty\"\n",
    "\n",
    "    Description = Soup.find(\"div\", {\"class\":\"recipe-description\"})\n",
    "    recipe[\"Description\"] = Description.get_text(strip = True) if Description else \"Empty\"\n",
    "    \n",
    "    Dietary = Soup.find(\"div\", {\"class\":\"recipe-metadata__dietary\"})\n",
    "    recipe[\"Dietary\"] = Dietary.get_text(strip = True) if Dietary else \"Empty\"\n",
    "\n",
    "\n",
    "    ingredients_body = Soup.find_all(\"div\", {\"class\":\"recipe-ingredients\"})[0]    \n",
    "    headers = []\n",
    "    headers.extend([i.string for i in ingredients_body.find_all(\"h2\")])\n",
    "    headers.extend([i.string for i in ingredients_body.find_all(\"h3\")])\n",
    "    ingredients_sub = ingredients_body.find_all(\"ul\")\n",
    "\n",
    "\n",
    "    Ingredient_list = {}\n",
    "    for i in range(len(ingredients_sub)):\n",
    "        if len(headers) != len(ingredients_sub):\n",
    "            Ingredient_list[headers[i+1]] = [\"\".join(j.strings) \n",
    "                                            for j in ingredients_sub[i].find_all(\"li\")]\n",
    "\n",
    "\n",
    "        else: \n",
    "            Ingredient_list[headers[i]] = [\"\".join(j.strings) \n",
    "                                        for j in ingredients_sub[i].find_all(\"li\")]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    recipe[\"Ingredient_list\"] = Ingredient_list\n",
    "    recipe[\"Instructions\"] = [\"\".join(j.stripped_strings) \n",
    "                            for j in Soup.find_all(\"li\", {\"itemprop\":\"recipeInstructions\"})]\n",
    "\n",
    "\n",
    "\n",
    "    return recipe\n",
    "\n",
    "\n",
    "def recipeWrite(filename, recipe):\n",
    "    '''Write recipe on disk as docID.txt'''\n",
    "\n",
    "    header = [\"Title\", \"Author\", \"CookTime\", \"Prep_Time\", \n",
    "             \"Serves\", \"Description\", \"Dietary\", \"Ingredient_list\", \n",
    "             \"Instructions\"]\n",
    "    s_first = \"\\t\".join([\"\".join(recipe[i]) for i in header[:7] ])\n",
    "\n",
    "    s_instr = \"\".join(recipe[\"Instructions\"])\n",
    "    s_ing = \"\"\n",
    "    for key in recipe[\"Ingredient_list\"].keys():\n",
    "        s_ing = s_ing + \"\\t\".join([i for i in recipe[\"Ingredient_list\"][key]])\n",
    "\n",
    "    s = s_first + \"\\t\" + s_instr + \"\\t\" + s_ing\n",
    "\n",
    "    f = open(filename,\"w\")\n",
    "    f.write(s)\n",
    "    f.close()    \n",
    "\n",
    "\n",
    "def buildCollection(recipesAddresses, delay = 1, last_num = 0):\n",
    "    \"\"\"\n",
    "    Parse every recipe on from http://www.bbc.co.uk/food.\n",
    "    Each repice is written in a tab separated text file with\n",
    "    a hierarchy of folders, in the folderPath.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(\"recipes\"):\n",
    "        os.makedirs(\"recipes\")\n",
    "    recipesAddresses = list(set(recipesAddresses))\n",
    "    for url in recipesAddresses:\n",
    "        #try: recipe = parseRecipe(url)\n",
    "        #except: \n",
    "        #    print(\"\\nParsing error occurred. At: \"+str(last_num)+\"\\n\")\n",
    "        #    return last_num\n",
    "        recipe = parseRecipe(url)\n",
    "        filename = \"recipes/\"+str(last_num).zfill(5)+\".txt\" \n",
    "        recipeWrite(filename, recipe)\n",
    "        print(str(last_num).zfill(5))\n",
    "        last_num +=1\n",
    "        time.sleep(delay)\n",
    "    return last_num\n",
    "\n",
    "# Linguistic preprocessing of recipe, see NLTK\n",
    "def processRecipe(path):\n",
    "    \"\"\"\n",
    "    Stopword removal, normalization, stemmings.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    stop = stopwords.words('english')\n",
    "    with open(path, encoding = \"utf-8\") as f:\n",
    "        s = f.read()\n",
    "\n",
    "    l = word_tokenize(s)\n",
    "    l = [i for word in l for i in word.split(\".\") if i]\n",
    "    l = [ps.stem(re.sub(r'[^a-zA-Z]', '', i.lower())) for i in word_tokenize(s) if i not in stop ]\n",
    "    l = list(filter(None, l))\n",
    "    l = [i for i in l if len(i)>2]\n",
    "    \n",
    "    return l\n",
    "\n",
    "# Term frequency in document\n",
    "def tf(path):\n",
    "    '''Compute the term frequency of a word in a document as\n",
    "    the number of the term appear in the document divided the total \n",
    "    count of the word in the document.'''\n",
    "    freq = {}\n",
    "    l = processRecipe(path)    # the document is preprocessed \n",
    "    tot_count = len(l)    # so we don't keep in consideration stopword etc.\n",
    "    for word in l:\n",
    "        if word in freq:\n",
    "            freq[word] += 1\n",
    "        if word not in freq:\n",
    "            freq[word] = 1\n",
    "\n",
    "    for key in freq.keys():\n",
    "        freq[key] = round(freq[key]/tot_count,4)\n",
    "    \n",
    "    return freq\n",
    "\n",
    "\n",
    "# Compile dictionary used to write vocabulary and index\n",
    "def recipeDict():\n",
    "    '''Create a dictionary who has parsed recipes keywords has keys,\n",
    "    a list of lists containing [path_to_file in which keyword appear, \n",
    "    frequency of the word in the document, position]. The frequency is\n",
    "    the tf (term frequency)'''\n",
    "    my_dict = {}\n",
    "    for file_path in os.listdir(\"recipes/\"):\n",
    "        freq = tf(\"recipes/\"+file_path)\n",
    "        word_list = processRecipe(\"recipes/\"+file_path)\n",
    "        \n",
    "        for word in set(word_list):\n",
    "            pos = [n for n, i in enumerate(word_list) if i == word]\n",
    "            if word in my_dict.keys():\n",
    "                my_dict[word].append([file_path.strip(\".txt\"), freq[word], pos]) \n",
    "                                      \n",
    "            if word not in my_dict.keys():\n",
    "                my_dict[word] = [[file_path.strip(\".txt\"),freq[word], pos]]\n",
    "            \n",
    "    return my_dict\n",
    "\n",
    "# Add skip pointers to index\n",
    "def add_skip(index):\n",
    "    '''Add skip to posting lists. Skip has step equal to square root\n",
    "    of posting list length.'''\n",
    "    for term in index.keys():\n",
    "        pos_length = len(index[term])\n",
    "        step = int(sqrt(pos_length))\n",
    "        for n in range(pos_length):\n",
    "            if n in range(0, pos_length-step, step): index[term][n].insert(2, n+step)\n",
    "            else : index[term][n].insert(2, 0)\n",
    "\n",
    "\n",
    "def vocabulary(my_dict):\n",
    "    '''Write the vocabulary on disk, each term has associated termID\n",
    "    and overall frequency (# documents occur / tot doc)'''\n",
    "    word_list = list(my_dict.keys())\n",
    "    word_list.sort()\n",
    "    word_num = len(os.listdir(\"./recipes/\"))\n",
    "    with open(\"vocabulary.txt\",\"w\") as f:\n",
    "        i = 0\n",
    "        for word in word_list:\n",
    "            idf = log(word_num/len(my_dict[word]))\n",
    "            f.write(word+\"\\t\" +str(i).zfill(len(str(word_num)))+ \"\\t\" +str(round(idf,3))+\"\\n\")\n",
    "            i+=1\n",
    "            \n",
    "# Write the index on disk\n",
    "def index(my_dict):\n",
    "    '''Write the index on disk.'''\n",
    "    vocabulary = {}\n",
    "    with open(\"vocabulary.txt\") as f:\n",
    "        for line in f:\n",
    "            l = line.split()\n",
    "            vocabulary[l[0]] = (l[1],l[2])\n",
    "    g = open(\"index.txt\", \"w\")\n",
    "    for word in vocabulary.keys():\n",
    "        g.write(vocabulary[word][0]+\"\\t\")\n",
    "        for file_ref in my_dict[word]:\n",
    "            pos = \"-\".join([str(i) for i in file_ref[3]])\n",
    "            tfidf = str( round( file_ref[1]*float(vocabulary[word][1]),3 ) )\n",
    "            g.write(file_ref[0]+ \" \" + tfidf + \" \" + str(file_ref[2]) + \" \" + pos +\"\\t\")\n",
    "        g.write(\"\\n\")\n",
    "    g.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### FUNCTIONS TO RECOVER DATA FROM THE HARD-DISK\n",
    "\n",
    "def retrieveRecipe(rec_num):\n",
    "    '''Retrieve a recipe from the disk give the recipe number.'''\n",
    "    headers = [\"Title\", \"Author\", \"CookTime\", \"Prep_Time\", \n",
    "                 \"Serves\", \"Description\", \"Dietary\", \"Instructions\", \n",
    "                 \"Ingredient_list\"]\n",
    "    path = \"recipes/\"+str(rec_num).zfill(5)+ \".txt\"\n",
    "    rec = pd.read_csv(path, sep = \"\\t\", header = None)\n",
    "\n",
    "    rec = rec.rename(columns = {i:headers[i] for i in range(0,8)})\n",
    "    rec = rec.rename(columns = {i:\"Ingredients\" \n",
    "                      for i in range(8,rec.shape[1])})\n",
    "    return rec\n",
    "\n",
    "def loadVocabulary():\n",
    "    '''Load Vocabulary File'''\n",
    "    vocabulary = {}\n",
    "    with open(\"vocabulary.txt\") as f:\n",
    "        for line in f:\n",
    "            l = line.split()\n",
    "            vocabulary[l[0]] = (l[1], l[2])\n",
    "    return vocabulary\n",
    "\n",
    "def loadIndex():\n",
    "    '''Load Index file.'''\n",
    "    index = {}\n",
    "    with open(\"index.txt\") as f:\n",
    "        for line in f:\n",
    "            l = line.split(\"\\t\")\n",
    "            l = [i.strip() for i in l if i.strip()]\n",
    "            l1 = [i.split() for i in l[1:]]\n",
    "            for i in range(len(l1)):    \n",
    "                pos = [int(i) for i in l1[i][3].split(\"-\")]\n",
    "                l1[i].pop(3)\n",
    "                l1[i].append(pos)\n",
    "            index[l[0]] = l1 \n",
    "    return index\n",
    "\n",
    "def posting(term, index, vocabulary):\n",
    "    '''Take a term and return its posting list.'''\n",
    "    term_num = vocabulary[term][0]\n",
    "    return index[term_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### FUNCTIONS TO PERFORM SEARCH AND OUTPUT RESULTS\n",
    "\n",
    "# Intersection of two posting lists\n",
    "def intersect(a,b, k):\n",
    "    '''Algorithm to intersect two posting lists. k is the distance\n",
    "    parameter in the proximity search.'''\n",
    "    i = 0\n",
    "    j = 0\n",
    "    answer = []\n",
    "\n",
    "    while j < len(b) and i < len(a):\n",
    "        if a[i][0] == b[j][0]:    # same filenumber\n",
    "            if k == 0:    # not proximity search\n",
    "                answer.append(a[i])\n",
    "            if k != 0 and proximitySearch(a[i][3], b[j][3],k)!=[]:\n",
    "                answer.append(a[i])\n",
    "            i +=1; j+=1\n",
    "        else:\n",
    "            if (int(a[i][0]) > int(b[j][0])):\n",
    "                skip = hasSkip(b, j)\n",
    "                if skip : j = int(skip) \n",
    "                else: j+=1\n",
    "            else:\n",
    "                skip = hasSkip(a, i)\n",
    "                if skip : i = int(skip) \n",
    "                else: i+=1\n",
    "    return answer\n",
    "\n",
    "def hasSkip(posting, pos):\n",
    "    '''Check if a posting list has a skip in a certain position.'''\n",
    "    skip = int(posting[pos][2]) \n",
    "    if skip != 0: return skip\n",
    "    else : return False\n",
    "\n",
    "def proximitySearch(pp1,pp2, k):\n",
    "    '''Perform a proximity search.'''\n",
    "    answer = []\n",
    "    l = []\n",
    "    ii = 0\n",
    "    jj = 0\n",
    "    while ii< len(pp1) :    # \n",
    "        while jj < len(pp2):\n",
    "            if abs(pp1[ii] - pp2[jj]) <= k:\n",
    "                l.append(pp2[jj])\n",
    "            elif pp2[jj] > pp1[ii]:\n",
    "                break\n",
    "            jj += 1\n",
    "        while l != [] and abs(l[0]-pp1[ii]) > k:\n",
    "            l.pop(0)\n",
    "        for i in l:\n",
    "            answer.append([pp1[ii]]+[i])\n",
    "        ii += 1\n",
    "    return answer\n",
    "    \n",
    "def processQuery(query):\n",
    "    \"\"\"\n",
    "    Stopword removal, normalization, stemmings.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    stop = stopwords.words('english')\n",
    "    l = word_tokenize(query)\n",
    "    l = [i for word in query for i in word.split(\".\") if i]\n",
    "    l = [ps.stem(re.sub(r'[^a-zA-Z]', '', i.lower())) \n",
    "         for i in word_tokenize(query) \n",
    "         if i not in stop ]\n",
    "    l = list(filter(None, l))\n",
    "    l = [i for i in l if len(i)>2]\n",
    "    if len(l) == 1 : l = str(*l)\n",
    "    return l\n",
    "\n",
    "\n",
    "def checkProx(s):\n",
    "    '''check if there query contains a proximity search with sintax [word1] /[distance] [word2]}.\n",
    "    Return the couples of word to search and the proximity range.'''\n",
    "    couples = {}\n",
    "    s = s.split(\"/\")\n",
    "    for i in range(len(s)-1):\n",
    "        couples[processQuery(s[i].split()[-1]), processQuery(s[i+1].split()[1])] = s[i+1].split()[0] \n",
    "    return couples\n",
    "\n",
    "\n",
    "def search(query, index, vocabulary):\n",
    "    '''Conjunctive search algorithm.'''\n",
    "    \n",
    "    couples = checkProx(query) # control if the query contain proximity\n",
    "    query = processQuery(query)    # process query like recipes\n",
    "    \n",
    "    if type(query) == str : query = [query]  # transform single query in list\n",
    "    \n",
    "    for term in query:    # remove terms not in vocabulary\n",
    "        if term not in voc.keys(): query.remove(term)\n",
    "    \n",
    "    sbf = sorted([(term, voc[term][1]) for term in query],\n",
    "                 reverse = True,\n",
    "                 key = lambda x : x[1] )\n",
    "    terms = []\n",
    "    if couples:    # if there is a proximity search\n",
    "        couples_temp = couples.copy()\n",
    "        for i,j in couples.keys():\n",
    "            if i or j not in voc.keys(): del couples_temp[i,j]\n",
    "        couples = couples_temp.copy()\n",
    "        for i,j in couples.keys():\n",
    "            sbf = [k for k in sbf if k[0] != i ]\n",
    "            sbf = [k for k in sbf if k[0] != j ]\n",
    "            a = posting(i, *X)\n",
    "            b = posting(j, *X)\n",
    "            terms.extend(intersect(a,b, int(couples[i,j])))\n",
    "    if sbf: \n",
    "        results = posting(sbf[0][0], index, vocabulary)\n",
    "        terms.extend([posting(term[0],index, vocabulary) for term in sbf[1:]])\n",
    "    else: return terms\n",
    "    while terms != [] and results != []:\n",
    "        results = intersect(results, \n",
    "                            terms[0], 0)\n",
    "        terms = terms[1:]   \n",
    "    return results\n",
    "\n",
    "\n",
    "def cosSim(query, docIDResults, ind, voc):\n",
    "    '''Cosine similarity between documents.'''    # To select search results.\n",
    "    N = len(docIDResults)\n",
    "    query = processQuery(query)\n",
    "    if type(query) == str : query = [query]\n",
    "    \n",
    "    for term in query:\n",
    "        if term not in voc.keys(): query.remove(term)\n",
    "\n",
    "    Scores = {i:0 for i in docIDResults}\n",
    "    Length = []\n",
    "    \n",
    "    for doc in docIDResults:\n",
    "        Length.append(len(processRecipe(\"recipes/\"+doc+\".txt\")))\n",
    "    \n",
    "    for term in query:\n",
    "        wtq = float(voc[term][1])  # idf\n",
    "        for i in ind[ voc[term][0] ]:\n",
    "            if i[0] in docIDResults:\n",
    "                Scores[i[0]] += wtq*float(i[1])  # idf-tf\n",
    "    for n,i in enumerate(docIDResults):\n",
    "        Scores[i] = Scores[i]/Length[n]\n",
    "    \n",
    "    Scores = sorted(Scores, key = Scores.get, reverse= True)\n",
    "    return Scores\n",
    "\n",
    "def printResults(results):\n",
    "    '''Printout search results.'''\n",
    "    for doc in results:\n",
    "        df = retrieveRecipe(doc)\n",
    "        print(\"Title: \"+df[\"Title\"][0])\n",
    "        print(\"Author: \"+ df[\"Author\"][0])\n",
    "        print(\"-\"*(len(df[\"Title\"][0])+7))\n",
    "        print(\"Description :\"+df[\"Description\"][0])\n",
    "        print(\"-\"*(len(df[\"Title\"][0])+7))\n",
    "        print(\"Cooking Time :\"+df[\"CookTime\"][0])\n",
    "        print(\"Preparation Time: \"+df[\"Prep_Time\"][0])\n",
    "        print(\"Serves: \"+df[\"Serves\"][0])\n",
    "        print(\"Dietary: \"+df[\"Dietary\"][0])\n",
    "        print(\"-\"*(len(df[\"Title\"][0])+7))\n",
    "        l = [i.strip() for i in df[\"Ingredients\"].to_csv().split(\"\\\"\") if not i.startswith(\"Ingredients\")  if i !=\"\"][1:]\n",
    "        l = [i.replace(\",\", \"\") for i in l if i]\n",
    "        print(\"Ingredient List:\\n\")\n",
    "        print(*l, sep = \"\\n\")\n",
    "        print(\"-\"*(len(df[\"Title\"][0])+7))\n",
    "        print(\"Instructions:\\n\"+df[\"Instructions\"][0])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## BUILDING COLLECTION\n",
    "\n",
    "getIngredients(write = True)\n",
    "getRecipeLinks(ingredients[304:], delay = 0, verbose = True)\n",
    "\n",
    "## Load Ingredient List\n",
    "\n",
    "with open(\"./ingredient_list.txt\") as f:\n",
    "    ingredients = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "## Load recipesAddresses\n",
    "\n",
    "with open(\"./recipe_links.txt\") as f:\n",
    "    recipes_add = list(set([line.strip() for line in f if line.strip()]))\n",
    "\n",
    "buildCollection(recipes_add, delay = 1)\n",
    "\n",
    "# Make dictionary and add skipping list. Then create vocabulary and index.\n",
    "rec_dic = recipeDict()  # 3.5 minuti\n",
    "add_skip(rec_dic)\n",
    "vocabulary(rec_dic)\n",
    "index(rec_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Index...\n",
      "\n",
      "Index loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Load Index and vocabulary\n",
    "\n",
    "print(\"Loading Index...\\n\")\n",
    "ind = loadIndex()    # load index in memory\n",
    "voc = loadVocabulary()    # load vocabulary in memory\n",
    "X = (ind, voc)    # create ind,voc tuple\n",
    "print(\"Index loaded.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search : "
     ]
    }
   ],
   "source": [
    "## SEARCH INTERFACE \n",
    "\n",
    "print(\"Search : \",end = '')\n",
    "query = str(input( ))    # input query\n",
    "\n",
    "searchResults = search(query, *X)\n",
    "docIDResults  = [d[0] for d in searchResults]\n",
    "\n",
    "print(\"Found \"+str(len(docIDResults))+ \" results.\\n\")\n",
    "\n",
    "max_results = 10    # set max number of result to output\n",
    "\n",
    "results = cosSim(query, docIDResults, *X)[:max_results]\n",
    "printResults(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
